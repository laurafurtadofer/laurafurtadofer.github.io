[
  {
    "objectID": "viz1.html",
    "href": "viz1.html",
    "title": "Hollywood Age Gaps",
    "section": "",
    "text": "Hollywood Age Gaps Over Time\n\n\n\n\nThis plot shows that couple age gaps have remained pretty consistent over time and that the data was skewed towards more recent movies. It also shows that there are many more non-same gender couples than same gender couples in Hollywood movies but that the same gender couples are equally spread out along the age gap axis. Same gender couples only appeared in this Hollywood data set after the year 2000.\nCitations:\nThis data comes from Hollywood Age Gap via Data Is Plural.\nData Science Learning Community (2024). Tidy Tuesday: A weekly social data project. https://tidytues.day\nhttps://github.com/rfordatascience/tidytuesday/tree/main/data/2023/2023-02-14"
  },
  {
    "objectID": "viz2.html",
    "href": "viz2.html",
    "title": "Valentines Gift Habits",
    "section": "",
    "text": "Valentines Gifts by Age Group\n\n\n\n\nThis plot shows that people’s shopping habits on valentines day stay pretty stable with age. The few differences are in the purchase of jewelry and greeting cards. The proportion of jewelry purchased compared to other items declines after people turn 44 and the proportion of greeting cards purchased picks up at age 35+.\nCitations:\nThis data comes from The National Retail Federation in the United States’ Valentine’s Day Data Center, specifically the Valentine’s Day survey data.\nData Science Learning Community (2024). Tidy Tuesday: A weekly social data project. https://tidytues.day\nhttps://github.com/rfordatascience/tidytuesday/tree/main/data/2024/2024-02-13"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Laura Furtado Fernandes",
    "section": "",
    "text": "Hi there! My name is Laura. I’m currently a Senior at Pomona College. I’m a Psychological Science Major and work as a research assistant at the MIC Lab and the PPPR Lab. I have two dogs and love going to the beach!"
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "SQL and large databases",
    "section": "",
    "text": "Show the code\nlibrary(tidyverse)\nlibrary(RMariaDB)\nlibrary(DBI)\n\n\n\n\nShow the code\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\nThe Stanford Open Policing Project compiled data from police stops from 42 different states across the United States. In this project, I analyze the racial make up of police stops in three major California cities—Los Angeles, San Diego, and San Francisco—to explore differences in stop patterns across locations and stop types. I will use SQL to access this data and wrangle it to examine my variables of interest: the race of the person stopped by police and the type of stop. I will limit my analyses to the year 2015.\n\n\nShow the code\nSELECT * FROM (\n  SELECT subject_race, type,   \n   SUM(1) AS num_stops,\n   'los angeles' AS city\n  FROM ca_los_angeles_2020_04_01\n  WHERE type IS NOT NULL\n    AND date BETWEEN '2015-01-01' AND '2015-12-31'\n  GROUP BY  subject_race, type\n\n  UNION ALL\n\n  SELECT subject_race, type,   \n    SUM(1) AS num_stops,\n    'san diego' AS city\n  FROM ca_san_diego_2020_04_01\n  WHERE type IS NOT NULL\n    AND date BETWEEN '2015-01-01' AND '2015-12-31'\n  GROUP BY  subject_race, type\n\n  UNION ALL\n\n  SELECT subject_race, type,   \n    SUM(1) AS num_stops,\n   'san francisco' AS city\n  FROM ca_san_francisco_2020_04_01\n  WHERE type IS NOT NULL\n    AND date BETWEEN '2015-01-01' AND '2015-12-31'\n  GROUP BY  subject_race, type\n) AS combined\nORDER BY num_stops DESC;\n\n\n\nDisplaying records 1 - 10\n\n\nsubject_race\ntype\nnum_stops\ncity\n\n\n\n\nhispanic\nvehicular\n164909\nlos angeles\n\n\nwhite\nvehicular\n87406\nlos angeles\n\n\nblack\nvehicular\n86184\nlos angeles\n\n\nhispanic\npedestrian\n60898\nlos angeles\n\n\nwhite\nvehicular\n48758\nsan diego\n\n\nblack\npedestrian\n42330\nlos angeles\n\n\nhispanic\nvehicular\n33609\nsan diego\n\n\nwhite\nvehicular\n29867\nsan francisco\n\n\nother\nvehicular\n29488\nlos angeles\n\n\nwhite\npedestrian\n24944\nlos angeles\n\n\n\n\n\nThe table above displays the number and type of police stops by race across three California cities, ordered in descending fashion by total number of stops. The plot below visualizes these results. Los Angeles recorded the highest number of stops overall (this is not surprising given that LA has more people than San Diego and San Francisco), with Hispanic individuals being stopped most frequently in both vehicular and pedestrian contexts. San Diego is a distant second in terms of overall stops, with San Francisco coming last in this comparison. In San Diego and San Francisco, white drivers are stopped most often, however, these results must be taken in consideration of the racial make up of the three cities.\nAccording to the U.S. Census Bureau, Los Angeles has a Hispanic majority, with approximately 47% of its population identifying as Hispanic. In contrast, San Diego’s population is majority White, with 50% identifying as White. Similarly, in San Francisco, White individuals also make up the largest racial group, comprising about 50% of the population.\nSan Francisco and San Diego lack data on pedestrian stops which limits this comparison.\n\n\nShow the code\ndf &lt;- dbGetQuery(con_traffic, \"\n  SELECT subject_race, type,   \n  SUM(1) AS num_stops,\n  'los angeles' AS city\nFROM ca_los_angeles_2020_04_01\nWHERE type IS NOT NULL\n  AND date BETWEEN '2015-01-01' AND '2015-12-31'\nGROUP BY  subject_race, type\n\nUNION ALL\n\nSELECT subject_race, type,   \n  SUM(1) AS num_stops,\n  'san diego' AS city\nFROM ca_san_diego_2020_04_01\nWHERE type IS NOT NULL\n  AND date BETWEEN '2015-01-01' AND '2015-12-31'\nGROUP BY  subject_race, type\n\nUNION ALL\n\nSELECT subject_race, type,   \n  SUM(1) AS num_stops,\n  'san francisco' AS city\nFROM ca_san_francisco_2020_04_01\nWHERE type IS NOT NULL\n  AND date BETWEEN '2015-01-01' AND '2015-12-31'\nGROUP BY  subject_race, type\n\")\n\nggplot(df, aes(x = subject_race, y = num_stops, fill = type)) + \n  geom_col(position = \"dodge\") + \n  labs(\n    title = \"Number of Stops by Race and Stop Type in Los Angeles, \\nSan Diego and San Francisco in 2015\",\n    x = \"Subject Race\",\n    y = \"Number of Stops\",\n    fill = \"Stop Type\"\n  ) +\n  theme_minimal() +\n  facet_wrap( . ~ city) + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nComparing within California is good to get a sense of within state differences in traffic stop patterns, however, these comparisons are not without limitations. For instance, Los Angeles is much more car reliant than San Francisco and also much more populous. San Diego might be more car-centric than San Francisco but Los Angeles experiences much higher traffic and has more extensive freeways than San Diego. Therefore, I decided to compare Los Angeles, California, to another highly car-centered city: Houston, Texas. It is important to note, however, that Houston (2.31 million people) is less populous than Los Angeles (3.8 million people).\n\n\nShow the code\nSELECT * FROM (\n  SELECT subject_race, type,   \n    SUM(1) AS num_stops,\n    'los angeles' AS city\n  FROM ca_los_angeles_2020_04_01\n  WHERE type IS NOT NULL\n    AND date BETWEEN '2015-01-01' AND '2015-12-31'\n  GROUP BY  subject_race, type\n\n  UNION ALL\n\n  SELECT subject_race, type,   \n    SUM(1) AS num_stops,\n   'houston' AS city\n  FROM tx_houston_2023_01_26\n  WHERE type IS NOT NULL\n    AND date BETWEEN '2015-01-01' AND '2015-12-31'\n  GROUP BY  subject_race, type\n) AS combined\nORDER BY num_stops DESC;\n\n\n\nDisplaying records 1 - 10\n\n\nsubject_race\ntype\nnum_stops\ncity\n\n\n\n\nwhite\nvehicular\n169470\nhouston\n\n\nhispanic\nvehicular\n164909\nlos angeles\n\n\nblack\nvehicular\n96645\nhouston\n\n\nwhite\nvehicular\n87406\nlos angeles\n\n\nblack\nvehicular\n86184\nlos angeles\n\n\nhispanic\npedestrian\n60898\nlos angeles\n\n\nNA\nvehicular\n46001\nhouston\n\n\nblack\npedestrian\n42330\nlos angeles\n\n\nother\nvehicular\n29488\nlos angeles\n\n\nwhite\npedestrian\n24944\nlos angeles\n\n\n\n\n\nThe table above displays the number and type of police stops by race in Los Angeles, CA, and Houston, TX, ordered in descending order by total stops (the plot below visualizes this data). Houston is missing data on pedestrian stops and on stops of Hispanic individuals. I investigated the table and confirmed that there is not a column on ethnicity. This missing data limits comparisons.\n\n\nShow the code\nSHOW COLUMNS FROM tx_houston_2023_01_26;\n\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\nlocation\ntext\nYES\n\nNA\n\n\n\nlat\ndouble\nYES\n\nNA\n\n\n\nlng\ndouble\nYES\n\nNA\n\n\n\ngeocode_source\ntext\nYES\n\nNA\n\n\n\nbeat\ntext\nYES\n\nNA\n\n\n\ndistrict\ntext\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\n\n\n\nCompared to other California cities, Houston and Los Angeles show more similar overall stop volumes. In Houston, White individuals are the most frequently stopped demographic, followed by Black individuals. In contrast, Los Angeles shows comparable numbers of vehicular stops for Black and White individuals, but White individuals are stopped less often in pedestrian encounters.\n\n\nShow the code\ndf &lt;- dbGetQuery(con_traffic, \"\n  SELECT subject_race, type,   \n    SUM(1) AS num_stops,\n    'los angeles' AS city\n  FROM ca_los_angeles_2020_04_01\n  WHERE type IS NOT NULL\n    AND date BETWEEN '2015-01-01' AND '2015-12-31'\n  GROUP BY  subject_race, type\n\n  UNION ALL\n\n  SELECT subject_race, type,   \n    SUM(1) AS num_stops,\n   'houston' AS city\n  FROM tx_houston_2023_01_26\n  WHERE type IS NOT NULL\n    AND date BETWEEN '2015-01-01' AND '2015-12-31'\n  GROUP BY  subject_race, type\n\")\n\nggplot(df, aes(x = subject_race, y = num_stops, fill = type)) + \n  geom_col(position = \"dodge\") + \n  labs(\n    title = \"Number of Stops by Race and Stop Type in Los Angeles and Houston in 2015\",\n    x = \"Subject Race\",\n    y = \"Number of Stops\",\n    fill = \"Stop Type\"\n  ) +\n  theme_minimal() +\n  facet_wrap( . ~ city) + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nIn this project, I used SQL to analyze police stop data from the Stanford Open Policing Project, focusing on racial patterns and stop types in three major California cities—Los Angeles, San Diego, and San Francisco—as well as a cross-state comparison with Houston, Texas. Within California, Los Angeles showed the highest number of stops, but the distribution of stops by race varied between cities. While comparing Los Angeles and Houston, I ran into an issue of missing data on the ethnicity (whether person stopped was of Hispanic origin) of the person stopped which limited comparisons with Los Angeles, which has a large hispanic population. I did see that Houston and Los Angeles have similar traffic stop counts, with makes sense given the car-centered culture of both cities.\nReferences:\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10.\nBureau, U. C. (n.d.). Census.gov | U.S. Census Bureau Homepage. Census.Gov. Retrieved April 29, 2025, from https://www.census.gov/en.html"
  },
  {
    "objectID": "project6.html#friends",
    "href": "project6.html#friends",
    "title": "End of Semester Presentation",
    "section": "Friends",
    "text": "Friends\nFriends is a famous sitcom that aired on NBC between 1994 and 2004. It followed the lives of 6 friends living in Manhattan and become a global pop culture sensation! Friends ran for 10 years and its scripts hold so much character data which can be fun to analyse!"
  },
  {
    "objectID": "project6.html#project-set-up",
    "href": "project6.html#project-set-up",
    "title": "End of Semester Presentation",
    "section": "Project Set Up",
    "text": "Project Set Up\nFor this project I used the following packages to examine data from the dialogue in friends episodes.\n\nlibrary(friends) \nlibrary(tidyverse)\nlibrary(RColorBrewer)\ndata(friends)\nas_tibble(friends)\n\n# A tibble: 67,373 × 6\n   text                                   speaker season episode scene utterance\n   &lt;chr&gt;                                  &lt;chr&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n 1 There's nothing to tell! He's just so… Monica…      1       1     1         1\n 2 C'mon, you're going out with the guy!… Joey T…      1       1     1         2\n 3 All right Joey, be nice. So does he h… Chandl…      1       1     1         3\n 4 Wait, does he eat chalk?               Phoebe…      1       1     1         4\n 5 (They all stare, bemused.)             Scene …      1       1     1         5\n 6 Just, 'cause, I don't want her to go … Phoebe…      1       1     1         6\n 7 Okay, everybody relax. This is not ev… Monica…      1       1     1         7\n 8 Sounds like a date to me.              Chandl…      1       1     1         8\n 9 [Time Lapse]                           Scene …      1       1     1         9\n10 Alright, so I'm back in high school, … Chandl…      1       1     1        10\n# ℹ 67,363 more rows"
  },
  {
    "objectID": "project6.html#how-you-doin",
    "href": "project6.html#how-you-doin",
    "title": "End of Semester Presentation",
    "section": "How you doin’?",
    "text": "How you doin’?\nOne of the characters from Friends, Joey Tribbiani, is famous for his catch phrase How you doin’"
  },
  {
    "objectID": "project6.html#how-did-i-do-it",
    "href": "project6.html#how-did-i-do-it",
    "title": "End of Semester Presentation",
    "section": "How did I do it?",
    "text": "How did I do it?\nI wanted to examine how often Joey says his catchphrase and if that changes throughout the seasons. To do that, I used the following R code.\n\nhowyoudoin &lt;- friends %&gt;% \n  filter(speaker == \"Joey Tribbiani\") %&gt;% \n  filter(str_detect(text, \"(?i)How you doin'\\\\?\")) %&gt;% \n  group_by(season) %&gt;% \n  summarise(howyoudoin_count = n()) %&gt;% \n  mutate(season = as.character(season)) \n\nhowyoudoinmerge &lt;- data.frame(\n  season = c(\"1\", \"2\", \"3\", \"10\"), \n  howyoudoin_count = c(0, 0, 0, 0)\n)\n\nhowyoudoin &lt;- bind_rows(howyoudoin, howyoudoinmerge) %&gt;% \n  mutate(season = factor(season, levels = sort(unique(as.numeric(season)))))"
  },
  {
    "objectID": "project6.html#how-each-season-is-doin",
    "href": "project6.html#how-each-season-is-doin",
    "title": "End of Semester Presentation",
    "section": "How each season is doin?",
    "text": "How each season is doin?\n\nggplot(howyoudoin, \n       aes(x = season, y = howyoudoin_count)) + \n  geom_col(aes(fill = season, colour = season)) + \n  labs(x = \"Season\", \n       y = \"Number of times Joey says 'how you doin'\", \n       title = \"How each season of Friends is doin'\") + \n  theme_minimal() + \n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "project6.html#smelly-friends",
    "href": "project6.html#smelly-friends",
    "title": "End of Semester Presentation",
    "section": "Smelly Friends…",
    "text": "Smelly Friends…\nAnother recurring joke in the show is Phoebe’s singing. She is a notoriously bad singer, with weird song subjects and lyrics on top of that. One of her most famous songs is “Smelly Cat”.\n\nPhoebe is also one of the most overlooked characters in the show. I wanted to see if in episodes with more mentions of “Smelly Cat” Phoebe would also have more lines."
  },
  {
    "objectID": "project6.html#how-did-i-do-it-1",
    "href": "project6.html#how-did-i-do-it-1",
    "title": "End of Semester Presentation",
    "section": "How did I do it?",
    "text": "How did I do it?\nI used the following code to count how many utterances per character (6 main friends) per episode and compared that to number of smelly cat mentions per episode.\n\nsmelly_cat &lt;- friends %&gt;% \n  mutate(smelly_cats = str_count(text, \"(?i)\\\\bsmelly cat\\\\b\")) %&gt;% \n  group_by(episode, season) %&gt;%\n  summarise(total_smelly_cats = sum(smelly_cats)) %&gt;% \n  ungroup()\n\nutterances_by_char &lt;- friends %&gt;% \n  group_by(season, episode, speaker) %&gt;%\n  summarise(total_utterances = n()) %&gt;% \n  ungroup() %&gt;% \n  filter(speaker == \"Chandler Bing\" | speaker == \"Monica Geller\" | speaker == \"Joey Tribbiani\" | speaker == \"Phoebe Buffay\" | speaker == \"Ross Geller\"| speaker == \"Rachel Green\")\n  \nsmelly_cats_char &lt;- left_join(utterances_by_char, smelly_cat)"
  },
  {
    "objectID": "project6.html#what-i-found",
    "href": "project6.html#what-i-found",
    "title": "End of Semester Presentation",
    "section": "What I found:",
    "text": "What I found:"
  },
  {
    "objectID": "project6.html#speaking-pattern-changes-over-the-avg-season",
    "href": "project6.html#speaking-pattern-changes-over-the-avg-season",
    "title": "End of Semester Presentation",
    "section": "Speaking pattern changes over the avg season",
    "text": "Speaking pattern changes over the avg season\nFriends was on for so long, it is possible that there were fluctuations in how much characters spoke per season or even over the course of a season. I examined the average utterance length for each of the main 6 friends over the course of the average season and per season.\n\nspeaking_len &lt;- friends %&gt;% \n  mutate(utterance_length = str_length(text)) %&gt;% \n  mutate(season = as.character(season)) %&gt;%\n  mutate(speaker = ifelse(speaker == \"Chandler Bing\" | speaker == \"Monica Geller\" | speaker == \"Joey Tribbiani\" | speaker == \"Phoebe Buffay\" | speaker == \"Ross Geller\"| speaker == \"Rachel Green\", speaker, \"Other\")) %&gt;%\n  group_by(episode, speaker) %&gt;% \n  summarise(avg_utt_length = mean(utterance_length, na.rm = TRUE)) %&gt;% \n  ungroup() %&gt;% \n  filter(!is.na(speaker), speaker != \"Other\")"
  },
  {
    "objectID": "project6.html#what-i-found-1",
    "href": "project6.html#what-i-found-1",
    "title": "End of Semester Presentation",
    "section": "What I found:",
    "text": "What I found:"
  },
  {
    "objectID": "project6.html#speaking-pattern-changes-over-the-series",
    "href": "project6.html#speaking-pattern-changes-over-the-series",
    "title": "End of Semester Presentation",
    "section": "Speaking pattern changes over the series",
    "text": "Speaking pattern changes over the series\n\nspeaking_len &lt;- friends %&gt;% \n  mutate(utterance_length = str_length(text)) %&gt;% \n  mutate(speaker = ifelse(speaker == \"Chandler Bing\" | speaker == \"Monica Geller\" | speaker == \"Joey Tribbiani\" | speaker == \"Phoebe Buffay\" | speaker == \"Ross Geller\"| speaker == \"Rachel Green\", speaker, \"Other\")) %&gt;%\n  group_by(season, speaker) %&gt;% \n  summarise(avg_utt_length = mean(utterance_length, na.rm = TRUE)) %&gt;% \n  ungroup() %&gt;% \n  filter(!is.na(speaker), speaker != \"Other\")"
  },
  {
    "objectID": "project6.html#what-i-found-2",
    "href": "project6.html#what-i-found-2",
    "title": "End of Semester Presentation",
    "section": "What I found:",
    "text": "What I found:"
  },
  {
    "objectID": "project6.html#oh-janice",
    "href": "project6.html#oh-janice",
    "title": "End of Semester Presentation",
    "section": "Oh Janice…",
    "text": "Oh Janice…\nWhile not one of the main friends, Janice is a memorable character. She is Chandler’s ex girlfriend and is famous for her horrible sounding laugh.\n\nI wanted to see who, among all the 6 friends was most often talking about Janice.\n\njanice &lt;- friends %&gt;%\n  mutate(Janices = str_count(text, \"(?&lt;=Janice)\\\\b\")) %&gt;%\n  filter(speaker == \"Chandler Bing\" | speaker == \"Monica Geller\" | speaker == \"Joey Tribbiani\" | speaker == \"Phoebe Buffay\" | speaker == \"Ross Geller\"| speaker == \"Rachel Green\" | speaker == \"Janice Litman Goralnik\" ) %&gt;% \n  group_by(speaker) %&gt;% \n  summarise(total_janices = sum(Janices))"
  },
  {
    "objectID": "project6.html#what-i-found-3",
    "href": "project6.html#what-i-found-3",
    "title": "End of Semester Presentation",
    "section": "What I found:",
    "text": "What I found:"
  },
  {
    "objectID": "project6.html#whose-voice-is-most-welcoming",
    "href": "project6.html#whose-voice-is-most-welcoming",
    "title": "End of Semester Presentation",
    "section": "Whose voice is most welcoming?",
    "text": "Whose voice is most welcoming?\nFinally, I was curious to see who is most often the first friend to speak in an episode.\n\nfirst &lt;- friends %&gt;% \n  filter(speaker != \"Scene Directions\") %&gt;% \n  group_by(season, episode) %&gt;% \n  slice_min(order_by = utterance, n = 1) %&gt;% \n  ungroup() %&gt;%\n  filter(scene == 1) %&gt;%\n  mutate(speaker = ifelse(speaker == \"Chandler Bing\" | speaker == \"Monica Geller\" | speaker == \"Joey Tribbiani\" | speaker == \"Phoebe Buffay\" | speaker == \"Ross Geller\"| speaker == \"Rachel Green\", speaker, \"Other\")) %&gt;%\n  group_by(speaker) %&gt;% \n  summarise(firstspeaker_count = n())"
  },
  {
    "objectID": "project6.html#what-i-found-4",
    "href": "project6.html#what-i-found-4",
    "title": "End of Semester Presentation",
    "section": "What I found:",
    "text": "What I found:"
  },
  {
    "objectID": "project6.html#the-friends-we-made-along-the-way",
    "href": "project6.html#the-friends-we-made-along-the-way",
    "title": "End of Semester Presentation",
    "section": "The Friends we made along the way…",
    "text": "The Friends we made along the way…\nI had a lot of fun with this project and liked learning how to use Regex!"
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Data Science Ethics",
    "section": "",
    "text": "Facebook Ethical Dilemma\nIn 2014, a paper titled ‘Experimental Evidence of Massive-Scale Emotional Contagion through Social Networks’ was published in Proceedings of the National Academy of Sciences of the United States of America (Kramer et al., 2014). This paper details the results from a large-scale experiment conducted on 689,003 English-speaking Facebook users to examine how the valence (positive or negative) of their News Feed content influenced their behavior on Facebook over a period of 1 week. Specifically, they measured whether people who saw differently valenced News Feeds used different percentages of positive vs negative words in their posts. This research, however, became highly controversial because it seemingly ignored pillars of academic investigation and data science, like informed consent, ethical data collection, improving lives, data transparency and availability, and generalizability.\nInformed Consent Violations\nFirstly, in order to do this research and observe emotion contagion, the researchers manipulated users’ news feeds and, in essence, experimented on them without their informed consent. Participants were recruited and randomly assigned to conditions without being informed of participation in the study (Kramer et al. 2014). There are obvious ethical issues with this, as manipulating participants without their consent goes against the Belmont Report’s guidelines for research on human participants (National Commission, 1979). Some argued that this sort of research is commonplace in industry (Park, 2014; Boesel, 2014) and that it only became unethical when it was published and fell short of the moral standards expected of academics (Park, 2014). Especially since it was done in collaboration with Cornell University: the Facebook researcher, Adam Kramer, partnered with two Cornell University investigators, Jamie Guillory and Jeffrey Hancock, in this project. The Cornell Institutional Review Board deemed the study exempt from their oversight because the data was being collected by Facebook, and some argue that it was a mistake (Boesel, 2014). This study violated the informed consent and ethical data collection principles of data science research.\nFailure to Improve Lives\nThe Facebook emotional contagion study also violated the data science principle that data should be used to improve the lives of individuals and communities. One could argue that this research is valuable and that the findings are beneficial to humanity because understanding emotional contagion is important and that this was the only way to study emotional contagion. However, that is not entirely true. Emotion contagion had been studied in more naturalistic ways years before Kramer et al. (2014) did it. Fowler & Christakis (2008) studied emotion contagion in social networks over a period of 20 years, 4739 participants consented to participate in a study; they found that there are clusters of happy and unhappy people in social networks and that happiness spread through the network, if a friend who lives within a mile becomes happy, it increases the probability that a person is happy by 25%. You could then argue that this effect is understudied in social media and that this was the only way to accurately do that, and that would also not be entirely true. One year after Kramer et al. (2014) published their study, and in response to the controversy, Ferrara and Yang (2015) investigated emotional contagion in social media by observing people on Twitter and measuring the valence of the content they had been exposed to without manipulating it. Therefore, the Facebook study not only introduced ethical harm (via emotional manipulation without consent) but also failed to generate new insights that justified those harms.\nLack of Transparency\nAdditionally, if this research had truly been revolutionary and overwhelmingly beneficial to others, then it stands to reason that the data collected in this study should be made publicly available and transparent which was not the case (Kramer et al., 2014). Instead, interested parties could reach out to Kramer et al. to request access to the data. This limits the work’s reproducibility and extension of the findings, violating data science principles.\nGeneralizability and Bias\nFurthermore, this research was conducted solely with English-speaking individuals who use Facebook, which raises concerns about cross-culture generalizability (Kramer et al., 2014). Then, can we claim that the findings are generalized to all of humanity? Or just to English-speaking, rich, industrialized populations? In this case, this research and findings are also perpetuating issues in the field of behavioral science, like the over-representation of WEIRD populations (Apa.Org, 2025). Given the access that Facebook had, they should and could have made this research more inclusive.\nReferences:\nAre your findings “WEIRD”? (n.d.). Https://Www.Apa.Org. Retrieved April 16, 2025, from https://www.apa.org/monitor/2010/05/weird\nBoesel, W. E. (2014, July 3). Facebook’s Controversial Experiment: Big Tech Is the New Big Pharma. TIME. https://time.com/2951726/facebook-emotion-contagion-experiment/\nFowler, J. H., & Christakis, N. A. (2008). Dynamic spread of happiness in a large social network: Longitudinal analysis over 20 years in the Framingham Heart Study. BMJ (Clinical Research Ed.), 337, a2338. https://doi.org/10.1136/bmj.a2338\nFerrara, E., & Yang, Z. (2015). Measuring Emotional Contagion in Social Media. PLOS ONE, 10(11), e0142390. https://doi.org/10.1371/journal.pone.0142390\nKramer, A. D., Guillory, J. E., and Hancock, J. T. (2014), Experimental Evidence of Massive-Scale Emotional Contagion through Social Networks, Proceedings of the National Academy of Sciences of the United States of America, 111, 8788–8790\nPark, A. (2014, June 30). Calm Down: Facebook Isn’t Really Manipulating Your Emotions. TIME. https://time.com/2941513/calm-down-facebook-isnt-manipulating-your-emotions/\nNational Commission for the Protection of Human Subjects of Biomedical and Behavioral Research. (1979). The Belmont report: Ethical principles and guidelines for the protection of human subjects of research. U.S. Department of Health and Human Services. https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/read-the-belmont-report/index.html"
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Text Analyses + Regex",
    "section": "",
    "text": "Show the code\nhowyoudoin &lt;- friends %&gt;% \n  filter(speaker == \"Joey Tribbiani\") %&gt;% \n  filter(str_detect(text, \"(?i)How you doin'\\\\?\")) %&gt;% \n  group_by(season) %&gt;% \n  summarise(howyoudoin_count = n()) %&gt;% \n  mutate(season = as.character(season)) \n\nhowyoudoinmerge &lt;- data.frame(\n  season = c(\"1\", \"2\", \"3\", \"10\"), \n  howyoudoin_count = c(0, 0, 0, 0)\n)\n\nhowyoudoin &lt;- bind_rows(howyoudoin, howyoudoinmerge) %&gt;% \n  mutate(season = factor(season, levels = sort(unique(as.numeric(season)))))\n\n\nggplot(howyoudoin, \n       aes(x = season, y = howyoudoin_count)) + \n  geom_col(aes(fill = season, colour = season)) + \n  labs(x = \"Season\", \n       y = \"Number of times Joey says 'how you doin'\", \n       title = \"How each season of Friends is doin'\") + \n  theme_minimal() + \n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nThis is a bar plot displaying how often Joey says “How you doin’?” per season of Friends. Joey first says his iconic catchphrase in season 4! It peaks during season 6 and by season 9 Joey barely says it anymore. This could reflect Joey’s character development as less of a ladies man or, that the writers grew tired of this joke.\n\n\nShow the code\nsmelly_cat &lt;- friends %&gt;% \n  mutate(smelly_cats = str_count(text, \"(?i)\\\\bsmelly cat\\\\b\")) %&gt;% \n  group_by(episode, season) %&gt;%\n  summarise(total_smelly_cats = sum(smelly_cats)) %&gt;% \n  ungroup()\n\nutterances_by_char &lt;- friends %&gt;% \n  group_by(season, episode, speaker) %&gt;%\n  summarise(total_utterances = n()) %&gt;% \n  ungroup() %&gt;% \n  filter(speaker == \"Chandler Bing\" | speaker == \"Monica Geller\" | speaker == \"Joey Tribbiani\" | speaker == \"Phoebe Buffay\" | speaker == \"Ross Geller\"| speaker == \"Rachel Green\")\n  \nsmelly_cats_char &lt;- left_join(utterances_by_char, smelly_cat)\n  \nggplot(smelly_cats_char, aes(x = total_utterances, y = total_smelly_cats)) + \n  geom_jitter(aes(colour = speaker, alpha = ifelse(total_smelly_cats == 0, 0.25, 1))) + \n  scale_alpha_identity() +  \n  labs(\n    title = \"Smelly Cat's Relationship to Character Dialogue\",\n    x = \"Total Utterances per Episode\",\n    y = \"Total 'Smelly Cat' Mentions per Episode\",\n    colour = \"Speaker\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis is a scatter plot displaying the total number of utterances per character (main 6 friends) in every episode of friends vs the total mentions of ‘Smelly Cat’. Surprisingly, on episodes with more ‘Smelly Cat’ mentions, Phoebe does not speak more often than the other friends.\n\n\nShow the code\nspeaking_len &lt;- friends %&gt;% \n  mutate(utterance_length = str_length(text)) %&gt;% \n  mutate(season = as.character(season)) %&gt;%\n  mutate(speaker = ifelse(speaker == \"Chandler Bing\" | speaker == \"Monica Geller\" | speaker == \"Joey Tribbiani\" | speaker == \"Phoebe Buffay\" | speaker == \"Ross Geller\"| speaker == \"Rachel Green\", speaker, \"Other\")) %&gt;%\n  group_by(episode, speaker) %&gt;% \n  summarise(avg_utt_length = mean(utterance_length, na.rm = TRUE)) %&gt;% \n  ungroup() %&gt;% \n  filter(!is.na(speaker), speaker != \"Other\")\n  \n\nggplot(speaking_len, aes(x = episode, y = avg_utt_length)) + \n  scale_colour_brewer(palette=\"Set1\") + \n  geom_point(aes(colour = speaker)) +  \n  geom_smooth(aes(colour = speaker), se = FALSE) +\n  labs(\n    x = \"Episodes\", \n    y = \"Average Utterance Length\", \n    title = \"Change in Friends Character Utterance Length Over an Average Season\"\n  ) +\n  theme_minimal() + \n  facet_wrap(~speaker)\n\n\n\n\n\n\n\n\n\nShow the code\nspeaking_len &lt;- friends %&gt;% \n  mutate(utterance_length = str_length(text)) %&gt;% \n  mutate(speaker = ifelse(speaker == \"Chandler Bing\" | speaker == \"Monica Geller\" | speaker == \"Joey Tribbiani\" | speaker == \"Phoebe Buffay\" | speaker == \"Ross Geller\"| speaker == \"Rachel Green\", speaker, \"Other\")) %&gt;%\n  group_by(season, speaker) %&gt;% \n  summarise(avg_utt_length = mean(utterance_length, na.rm = TRUE)) %&gt;% \n  ungroup() %&gt;% \n  filter(!is.na(speaker), speaker != \"Other\")\n\nggplot(speaking_len, aes(x = season, y = avg_utt_length)) + \n  scale_colour_brewer(palette=\"Set1\") + \n  geom_point(aes(colour = speaker)) + \n  geom_smooth(aes(colour = speaker), se = FALSE) + \n  scale_x_continuous(limits = c(1, 10), breaks = seq(1, 10, by = 1)) +\n  labs(x = \"Season\", \n       y = \"Average Utterance Length\", \n       title = \"Change in Friends Character Utterance Length Over the Series\") +\n  theme_minimal() + \n  facet_wrap(~speaker)\n\n\n\n\n\n\n\n\n\nThese are scatter plots with trend-lines displaying how average utterance length changes over time for each speaker, in both the span of the average season (per episode) and over the series (per season). Over the course of an average season of Friends, the characters utterances seem to get shorter (except for Chandler and Phoebe). Most characters utterance lengths fluctuated throughout the seasons. Notably, in season 1, Monica starts out with the shortest utterences and she slowly starts speaking longer utterances from season five onwards.\n\n\nShow the code\njanice &lt;- friends %&gt;%\n  mutate(Janices = str_count(text, \"(?&lt;=Janice)\\\\b\")) %&gt;%\n  filter(speaker == \"Chandler Bing\" | speaker == \"Monica Geller\" | speaker == \"Joey Tribbiani\" | speaker == \"Phoebe Buffay\" | speaker == \"Ross Geller\"| speaker == \"Rachel Green\" | speaker == \"Janice Litman Goralnik\" ) %&gt;% \n  group_by(speaker) %&gt;% \n  summarise(total_janices = sum(Janices))\n\n\nggplot(janice, \n       aes(x = speaker, y = total_janices)) + \n  geom_col(aes(fill = speaker)) + \n  scale_fill_brewer(palette=\"GnBu\") + \n  labs(x = \"Speaker\", \n       y = \"Times Referenced Janice\", \n       title = \"Who is most burdened by Janice?\") + \n  theme_minimal() + \n  theme(legend.position=\"none\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThis is a bar plot reflecting how often the characters in Friends refer to Janice. Chandler refers to Janice the most out of all his friends, this makes sense since he dates her throughout the series. Rachel is the character least burdened by Janice which must surely result in mental or ear health benefits.\n\n\nShow the code\nfirst &lt;- friends %&gt;% \n  filter(speaker != \"Scene Directions\") %&gt;% \n  group_by(season, episode) %&gt;% \n  slice_min(order_by = utterance, n = 1) %&gt;% \n  ungroup() %&gt;%\n  filter(scene == 1) %&gt;%\n  mutate(speaker = ifelse(speaker == \"Chandler Bing\" | speaker == \"Monica Geller\" | speaker == \"Joey Tribbiani\" | speaker == \"Phoebe Buffay\" | speaker == \"Ross Geller\"| speaker == \"Rachel Green\", speaker, \"Other\")) %&gt;%\n  group_by(speaker) %&gt;% \n  summarise(firstspeaker_count = n())\n\nggplot(first, \n       aes(x = speaker, y = firstspeaker_count)) + \n  geom_col(aes(fill = speaker)) + \n  scale_fill_brewer(palette=\"RdPu\") + \n  labs(x = \"Speaker\", \n       y = \"Times first to speak in an episode\", \n       title = \"Whose voice do you hear first in a Friends episode?\") + \n  theme_minimal() + \n  theme(legend.position=\"none\") \n\n\n\n\n\n\n\n\n\nThis is a bar plot showing how often the first line in an episode is spoken by each friend or generic other character. Joey is most often the first to speak, followed by Chandler. This probably reflects the writers desire to start on a comic relief moment and Joey is the silliest character.\nCitations:\nThis data comes from emilhvitfeldt.github.io/friends/ + Friends [TV series]. National Broadcasting Company."
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Permutation Testing",
    "section": "",
    "text": "Athletes tend to perform better at home, and this well-known phenomenon is called “home advantage”. It happens due to a combination of fan support, familiarity with the venue, no travel stress, etc. Clube de Regatas do Flamengo (better known as Flamengo) is a soccer club in Rio de Janeiro, Brazil and a fascinating case study in this context. With an estimated 40 to 50 million supporters spread across the country, Flamengo often draws large crowds regardless of location — even in away matches. In this project, I aim to investigate whether the traditional “home advantage” also applies to Flamengo or if their massive nationwide fanbase neutralizes the traditional home-field advantage. In other words: Does Flamengo play “at home” wherever they go? Or does playing in the iconic Maracanã — the most famous stadium in the world — still provide a meaningful edge? I will be measuring performance by comparing the number of goals scored by Flamengo at home vs. away using a dataset of brazilian soccer matches from the main brazilian leagues.\nI hypothesize that the difference in goals between home and away games for Flamengo can be attributed to chance because of the large fanbase.\n\n\nShow the code\nflamengo_games %&gt;%\n   mutate(\n      is_home = ifelse(home == \"Flamengo\", TRUE, FALSE),\n      goals_for = ifelse(is_home, home_goal, away_goal)\n    )  %&gt;%\n  group_by(is_home) %&gt;% \n  summarize(mean_goals = mean(goals_for)) %&gt;% \n  ggplot(aes(x = is_home, y = mean_goals, fill = is_home)) +\n  geom_col(width = 0.6) +\n  scale_fill_brewer(palette = \"PuRd\") + \n  scale_x_discrete(labels = c(\"Away\", \"Home\")) +\n  labs(\n    title = \"Flamengo's Average Goals Scored: Home vs. Away\",\n    x = \"Game Location\",\n    y = \"Average Goals Scored\", \n    fill = \"Playing at Home?\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot highlights the difference between Flamengo’s average goals scored at home vs away, Flamengo scores on average 1.75 goals in home matches, while scoring around 1.34 goals in away games. It is a difference of less than one goal.\n\n\nShow the code\nperm_data &lt;- function(rep, team){\n  games &lt;- br_soccer %&gt;%\n    filter(home == team | away == team) %&gt;%\n    mutate(\n      is_home = ifelse(home == team, TRUE, FALSE),\n      goals_for = ifelse(is_home, home_goal, away_goal)\n    ) %&gt;%\n    select(is_home, goals_for)\n\n  obs_diff &lt;- games %&gt;%\n    group_by(is_home) %&gt;%\n    summarize(mean_goals = mean(goals_for)) %&gt;%\n    summarize(diff = mean_goals[is_home == TRUE] - mean_goals[is_home == FALSE]) %&gt;%\n    pull(diff)\n\n  permuted &lt;- games %&gt;%\n    mutate(is_home_perm = sample(is_home))\n\n  perm_diff &lt;- permuted %&gt;%\n    group_by(is_home_perm) %&gt;%\n    summarize(mean_goals = mean(goals_for)) %&gt;%\n    summarize(diff = mean_goals[is_home_perm == TRUE] - mean_goals[is_home_perm == FALSE]) %&gt;%\n    pull(diff)\n\n  tibble(rep = rep, obs_diff = obs_diff, perm_diff = perm_diff)\n}\n\nperm_results &lt;- map(1:1000, perm_data, team = \"Flamengo\") |&gt; \n  list_rbind()\n\n\n\n\nShow the code\nggplot(perm_results, aes(x = perm_diff)) +\n  geom_histogram(bins = 30, fill = \"black\", color = \"red\") +\n  geom_vline(aes(xintercept = obs_diff), color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Permutation Test: Flamengo's Home vs. Away Goal Average\",\n    x = \"Permuted Mean Difference (Home - Away)\",\n    y = \"Count\"\n  ) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe permutation test revealed that Flamengo does, in fact, experience a home-field advantage. This rejects my hypothesis. As shown in the plot above, the difference in average goals scored between home and away matches (represented by the blue dashed line) is much greater than what would be expected by random chance. This provides strong evidence that Flamengo scores more goals at home than away, suggesting that playing in Maracanã may offer a meaningful boost.\nCitation:\nThis data was obtained from the Kaggle Brazilian Football Matches dataset: https://www.kaggle.com/datasets/cuecacuela/brazilian-football-matches"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is part of my Foundations of Data Science in R course at Pomona College. The analyses posted here are part of course projects!"
  }
]